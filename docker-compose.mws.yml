version: "3.8"

services:

  db:
    image: postgres:13.4
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 60
    labels:
      - autoheal=true
    user: postgres
    volumes:
      - db-pgdata-var:/var/lib/postgresql/data
      # DB initialization scripts
      - .dockerfiles/db/initdb.d/:/docker-entrypoint-initdb.d/
    networks:
      - intranet
    environment:
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      WBIA_DB_NAME: "${WBIA_DB_NAME}"
      WBIA_DB_USER: "${WBIA_DB_USER}"
      WBIA_DB_PASSWORD: "${WBIA_DB_PASSWORD}"
      WILDBOOK_DB_NAME: "${WILDBOOK_DB_NAME}"
      WILDBOOK_DB_USER: "${WILDBOOK_DB_USER}"
      WILDBOOK_DB_PASSWORD: "${WILDBOOK_DB_PASSWORD}"
      HOUSTON_DB_NAME: "${HOUSTON_DB_NAME}"
      HOUSTON_DB_USER: "${HOUSTON_DB_USER}"
      HOUSTON_DB_PASSWORD: "${HOUSTON_DB_PASSWORD}"

  redis:
    image: redis:latest
    command: ["redis-server", "/redis.conf"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 60
    labels:
      - autoheal=true
    volumes:
      - redis-var:/data
      - .dockerfiles/redis/redis.conf:/redis.conf
    networks:
      - intranet

  elasticsearch:
    # https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl --silent --fail 127.0.0.1:9200/_cluster/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 60
    labels:
      - autoheal=true
    volumes:
      - es-var1:/usr/share/elasticsearch/data
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      - intranet
    ports:
      # development exposure, not exposed in production
      - 9200:9200
      - 9300:9300
    environment:
      - node.name=elasticsearch
      - discovery.seed_hosts=elasticsearch2,elasticsearch3
      - cluster.initial_master_nodes=elasticsearch,elasticsearch2,elasticsearch3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"

  elasticsearch2:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    depends_on:
      - elasticsearch
    healthcheck:
      test: ["CMD-SHELL", "curl --silent --fail 127.0.0.1:9200/_cluster/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 60
    labels:
      - autoheal=true
    volumes:
      - es-var2:/usr/share/elasticsearch/data
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      - intranet
    environment:
      - node.name=elasticsearch2
      - discovery.seed_hosts=elasticsearch,elasticsearch3
      - cluster.initial_master_nodes=elasticsearch,elasticsearch2,elasticsearch3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"

  elasticsearch3:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    depends_on:
      - elasticsearch2
    healthcheck:
      test: ["CMD-SHELL", "curl --silent --fail 127.0.0.1:9200/_cluster/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 60
    labels:
      - autoheal=true
    volumes:
      - es-var3:/usr/share/elasticsearch/data
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      - intranet
    environment:
      - node.name=elasticsearch3
      - discovery.seed_hosts=elasticsearch,elasticsearch2
      - cluster.initial_master_nodes=elasticsearch,elasticsearch2,elasticsearch3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"

  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.0
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl --silent --fail 127.0.0.1:5601/status || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 60
    labels:
      - autoheal=true
    networks:
      - intranet
    ports:
      - 5601:5601
    environment:
      ELASTICSEARCH_HOSTS: "http://${ELASTICSEARCH_HOSTS}"

  sage:
    # https://github.com/WildMeOrg/wildbook-ia
    image: wildme/wbia:latest
    command: ["--db-uri", "${WBIA_DB_URI}"]
    depends_on:
      db:
        condition: service_healthy
    # healthcheck:  # WBIA defines it's own health check and is already labeled for autoheal
    # labels:
    #   - autoheal=true
    volumes:
      - sage-database-var:/data/db
      - sage-cache-var:/cache
    networks:
      - intranet
    ports:
      # FIXME: exposed for developer verification
      - "82:5000"
    environment:
      WBIA_DB_URI: "${WBIA_DB_URI}"
      HOUSTON_CLIENT_ID: "${HOUSTON_CLIENT_ID}"
      HOUSTON_CLIENT_SECRET: "${HOUSTON_CLIENT_SECRET}"

  sage-sync:
    # https://github.com/WildMeOrg/wildbook-ia
    image: wildme/wbia:latest
    command: ["--db-uri", "${WBIA_DB_URI}"]
    depends_on:
      sage:
        condition: service_healthy
      db:
        condition: service_healthy
    # healthcheck:  # WBIA defines it's own health check and is already labeled for autoheal
    # labels:
    #   - autoheal=true
    volumes:
      - sage-database-var:/data/db
      - sage-cache-var:/cache
    networks:
      - intranet
    environment:
      WBIA_DB_URI: "${WBIA_DB_URI}"
      HOUSTON_CLIENT_ID: "${HOUSTON_CLIENT_ID}"
      HOUSTON_CLIENT_SECRET: "${HOUSTON_CLIENT_SECRET}"

  houston:
    # https://github.com/WildMeOrg/houston
    image: wildme/houston:latest
    build: &houston-build
      context: .
      target: main
    command: ["invoke", "app.run"]
    depends_on:
      db:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ['CMD', '/docker-healthcheck.sh']
      interval: 10s
      timeout: 5s
      retries: 60
    labels:
      - autoheal=true
    volumes: &houston-volumes
      - houston-var:/data
      # These are added for development. Do not mount these in production.
      - .dockerfiles/docker-entrypoint.sh:/docker-entrypoint.sh
      - .dockerfiles/docker-healthcheck.sh:/docker-healthcheck.sh
      - .dockerfiles/docker-entrypoint-init.d.mws:/docker-entrypoint-init.d
      - .dockerfiles/docker-entrypoint-always-init.d:/docker-entrypoint-always-init.d
      # FIXME: pull in development code while working on bringing up the container
      - .:/code
    networks:
      - intranet
      - frontend
    ports:
      # FIXME: exposed for developer verification
      - "83:5000"
    environment: &houston-environment
      HOUSTON_APP_CONTEXT: mws
      # exposed service is 'localhost' using port 84
      SERVER_NAME: "${SERVER_NAME}"
      PREFERRED_URL_SCHEME: "${PREFERRED_URL_SCHEME}"
      FLASK_ENV: development
      SQLALCHEMY_DATABASE_URI: "${SQLALCHEMY_DATABASE_URI}"
      TEST_DATABASE_URI: "${TEST_DATABASE_URI}"
      SAGE_AUTHENTICATIONS_URI__DEFAULT: "${SAGE_AUTHENTICATIONS_URI__DEFAULT}"
      SAGE_AUTHENTICATIONS_URI__SYNC: "${SAGE_AUTHENTICATIONS_URI__SYNC}"
      ELASTICSEARCH_HOSTS: "${ELASTICSEARCH_HOSTS}"
      REDIS_HOST: redis
      REDIS_PASSWORD: "seekret_development_password"
      GITLAB_PROTO: "${GITLAB_PROTO}"
      GITLAB_HOST: "${GITLAB_HOST}"
      GITLAB_PORT: "${GITLAB_PORT}"
      GITLAB_ADMIN_PASSWORD: "${GITLAB_ADMIN_PASSWORD}"
      GITLAB_REMOTE_URI: "${GITLAB_REMOTE_URI}"
      GITLAB_REMOTE_LOGIN_PAT: "${GITLAB_REMOTE_LOGIN_PAT}"
      GITLAB_NAMESPACE: "${GITLAB_NAMESPACE}"
      GIT_SSH_KEY: "${GIT_SSH_KEY}"
      GIT_PUBLIC_NAME: "${GIT_PUBLIC_NAME}"
      GIT_EMAIL: "${GIT_EMAIL}"
      OAUTH_CLIENT_ID: "${HOUSTON_CLIENT_ID}"
      OAUTH_CLIENT_SECRET: "${HOUSTON_CLIENT_SECRET}"
      OAUTH_USER_EMAIL: "${OAUTH_USER_EMAIL}"
      WILDBOOK_DB_HOST: "${WILDBOOK_DB_HOST}"
      WILDBOOK_DB_NAME: "${WILDBOOK_DB_NAME}"
      WILDBOOK_DB_USER: "${WILDBOOK_DB_USER}"
      WILDBOOK_DB_PASSWORD: "${WILDBOOK_DB_PASSWORD}"
      LOG_WIDTH: ${LOG_WIDTH}
      TRANSLOADIT_KEY: "${TRANSLOADIT_KEY}"
      TRANSLOADIT_TEMPLATE_ID: "${TRANSLOADIT_TEMPLATE_ID}"
      TRANSLOADIT_SERVICE: "${TRANSLOADIT_SERVICE}"
      GOOGLE_MAPS_API_KEY: "${GOOGLE_MAPS_API_KEY}"
      SENTRY_DSN_DEVELOPMENT: "${SENTRY_DSN_DEVELOPMENT}"
      SENTRY_DSN_PRODUCTION: "${SENTRY_DSN_PRODUCTION}"
      FLATFILE_KEY: "${FLATFILE_KEY}"
      RECAPTCHA_PUBLIC_KEY: "${RECAPTCHA_PUBLIC_KEY}"
      RECAPTCHA_SECRET_KEY: "${RECAPTCHA_SECRET_KEY}"
      DEFAULT_EMAIL_SERVICE: "${DEFAULT_EMAIL_SERVICE}"
      DEFAULT_EMAIL_SERVICE_USERNAME: "${DEFAULT_EMAIL_SERVICE_USERNAME}"
      DEFAULT_EMAIL_SERVICE_PASSWORD: "${DEFAULT_EMAIL_SERVICE_PASSWORD}"

  celery-beat:
    image: wildme/houston:latest
    build: *houston-build
    command: ["celery", "-A", "app.extensions.celery.celery", "beat", "-s", "/data/var/celerybeat-schedule", "-l", "INFO"]
    depends_on:
      houston:
        condition: service_healthy
    healthcheck: &celery-healthcheck
      test: [ "CMD", "celery", "-A", "app.extensions.celery.celery", "status"]
      interval: 30s
      timeout: 15s
      retries: 60
    labels:
      - autoheal=true
    volumes: *houston-volumes
    networks:
      - intranet
    environment: *houston-environment

  celery-worker:
    image: wildme/houston:latest
    build: *houston-build
    command: ["celery", "-A", "app.extensions.celery.celery", "worker", "-l", "INFO", "--pool=gevent", "--task-events"]
    depends_on:
      houston:
        condition: service_healthy
    healthcheck: *celery-healthcheck
    labels:
      - autoheal=true
    volumes: *houston-volumes
    networks:
      - intranet
    environment: *houston-environment

  celery-worker2:
    image: wildme/houston:latest
    build: *houston-build
    command: ["celery", "-A", "app.extensions.celery.celery", "worker", "-l", "INFO", "--pool=gevent", "--task-events"]
    depends_on:
      houston:
        condition: service_healthy
    healthcheck: *celery-healthcheck
    labels:
      - autoheal=true
    volumes: *houston-volumes
    networks:
      - intranet
    environment: *houston-environment

  celery-worker3:
    image: wildme/houston:latest
    build: *houston-build
    command: ["celery", "-A", "app.extensions.celery.celery", "worker", "-l", "INFO", "--pool=gevent", "--task-events"]
    depends_on:
      houston:
        condition: service_healthy
    healthcheck: *celery-healthcheck
    labels:
      - autoheal=true
    volumes: *houston-volumes
    networks:
      - intranet
    environment: *houston-environment

  flower:
    image: wildme/houston:latest
    build: *houston-build
    command: ["celery", "-A", "app.extensions.celery.celery", "flower", "--basic_auth=${FLOWER_USER}:${FLOWER_PASSWORD}"]
    depends_on:
      houston:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://${FLOWER_USER}:${FLOWER_PASSWORD}@127.0.0.1:5555/metrics"]
      interval: 10s
      timeout: 5s
      retries: 60
    labels:
      - autoheal=true
    volumes: *houston-volumes
    networks:
      - intranet
    environment: *houston-environment
    ports:
      - "86:5555"

  dev-frontend:
    # this component is intended to only be used in development
    image: node:lts
    working_dir: /code
    entrypoint: "/docker-entrypoint.sh"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://127.0.0.1:3000/"]
      interval: 10s
      timeout: 5s
      retries: 60
    labels:
      - autoheal=true
    volumes:
      - .dockerfiles/dev-frontend/docker-entrypoint.sh:/docker-entrypoint.sh
      - ./:/code
    networks:
      - intranet
    environment:
      HOUSTON_APP_CONTEXT: mws
      # See port served by 'www' component (i.e. the reverse proxy)
      HOST: "0.0.0.0"
      PORT: "84"

  localhost:
    image: nginx:latest
    depends_on:
      # sage:
      #   condition: service_healthy
      houston:
        condition: service_healthy
      dev-frontend:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:84/"]
      interval: 10s
      timeout: 5s
      retries: 60
    labels:
      - autoheal=true
    volumes:
      - .dockerfiles/www/nginx.conf:/etc/nginx/conf.d/default.conf
    networks:
      - intranet
      - frontend
    ports:
      # BBB deprecated in favor or port 80, remains for backward compat
      - "84:80"

  autoheal:
    image: willfarrell/autoheal
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      AUTOHEAL_CONTAINER_LABEL: "autoheal"
      AUTOHEAL_INTERVAL: 15
      AUTOHEAL_START_PERIOD: 600
      AUTOHEAL_DEFAULT_STOP_TIMEOUT: 60
    restart: always

networks:
  intranet:
  frontend:

volumes:
  db-pgdata-var:
  es-var1:
  es-var2:
  es-var3:
  redis-var:
  sage-database-var:
  sage-cache-var:
  houston-var:
